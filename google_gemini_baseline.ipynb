{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # 운영체제와 상호작용하기 위한 모듈, 파일 및 디렉토리 작업을 수행할 수 있음\n",
    "\n",
    "import json  # JSON 데이터 처리 모듈, JSON 형식의 데이터를 파이썬 객체로 변환하거나 그 반대 작업을 수행할 수 있음\n",
    "\n",
    "from elasticsearch import Elasticsearch, helpers  # Elasticsearch 클라이언트와 헬퍼 함수 모듈 가져오기\n",
    "# Elasticsearch: 분산형 검색 및 분석 엔진과 상호작용하기 위한 클라이언트\n",
    "# helpers: Elasticsearch와의 상호작용을 간소화하기 위한 헬퍼 함수, 예: 대량 데이터 인덱싱을 위한 bulk() 함수 포함\n",
    "\n",
    "from sentence_transformers import SentenceTransformer  # 문장 임베딩을 위한 Transformer 모델 가져오기\n",
    "# SentenceTransformer: 사전 훈련된 Transformer 모델을 사용하여 문장을 고차원 벡터로 변환하는 기능 제공\n",
    "# 문서 유사도 계산, 검색 시스템, 문장 분류 등 다양한 NLP 작업에 활용 가능\n",
    "import google.generativeai as genai\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# # Sentence Transformer 모델 초기화 (한국어 임베딩 생성 가능한 어떤 모델도 가능)\n",
    "# model = SentenceTransformer(\"snunlp/KR-SBERT-V40K-klueNLI-augSTS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SetntenceTransformer를 이용하여 임베딩 생성\n",
    "def get_embedding(sentences):\n",
    "    client = OpenAI(\n",
    "    base_url=\"https://api.upstage.ai/v1/solar\"\n",
    "    )\n",
    "    batch_embeddings = []\n",
    "    query_result = client.embeddings.create(\n",
    "    model = \"embedding-query\",\n",
    "    input = sentences\n",
    "        )\n",
    "    for query_embedding in query_result.data:\n",
    "        batch_embeddings.append(query_embedding.embedding)\n",
    "    return np.array(batch_embeddings).astype('float32')\n",
    "\n",
    "# 주어진 문서의 리스트에서 배치 단위로 임베딩 생성\n",
    "def get_embeddings_in_batches(docs, batch_size=100):\n",
    "    batch_embeddings = []\n",
    "    for i in range(0, len(docs), batch_size):\n",
    "        batch = docs[i:i + batch_size]\n",
    "        contents = [doc[\"content\"] for doc in batch]\n",
    "        embeddings = get_embedding(contents)\n",
    "        batch_embeddings.extend(embeddings)\n",
    "    return batch_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_username = \"elastic\"\n",
    "es_password = \"\"\n",
    "\n",
    "# Elasticsearch client 생성\n",
    "es = Elasticsearch(['https://localhost:9200'], basic_auth=(es_username, es_password), ca_certs=\"/home/code/elasticsearch-8.15.2/config/certs/http_ca.crt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 색인을 위한 설정 설정\n",
    "settings = {\n",
    "    \"analysis\": {\n",
    "        \"analyzer\": {\n",
    "            \"nori\": {  # 사용자 정의 분석기 이름\n",
    "                \"type\": \"custom\",  # 분석기의 유형\n",
    "                \"tokenizer\": \"nori_tokenizer\",  # 사용할 토크나이저\n",
    "                \"decompound_mode\": \"mixed\",  # 복합어 분해 모드 설정 (mixed는 복합어를 분해하고, 그렇지 않은 경우는 그대로 유지)\n",
    "                \"filter\": [\"nori_posfilter\"]  # 적용할 필터 리스트\n",
    "            }\n",
    "        },\n",
    "        \"filter\": {\n",
    "            \"nori_posfilter\": {  # 사용자 정의 필터 이름\n",
    "                \"type\": \"nori_part_of_speech\",  # 품사 기반 필터 유형\n",
    "                # 어미, 조사, 구분자, 줄임표, 지정사, 보조 용언 등을 제거하기 위한 설정\n",
    "                \"stoptags\": [\n",
    "                    \"E\",  # 어미\n",
    "                    \"J\",  # 조사\n",
    "                    \"SC\",  # 구분자\n",
    "                    \"SE\",  # 줄임표\n",
    "                    \"SF\",  # 지정사\n",
    "                    \"VCN\",  # 연결 용언\n",
    "                    \"VCP\",  # 보조 용언\n",
    "                    \"VX\"   # 보조 용언\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 색인을 위한 매핑 설정 (역색인 필드, 임베딩 필드 모두 설정)\n",
    "mappings = {\n",
    "    \"properties\": {\n",
    "        \"content\": {  # 텍스트 필드 설정\n",
    "            \"type\": \"text\",  # 필드 타입을 텍스트로 설정\n",
    "            \"analyzer\": \"nori\"  # nori 분석기를 사용하여 텍스트 분석\n",
    "        },\n",
    "        \"embeddings\": {  # 임베딩 필드 설정\n",
    "            \"type\": \"dense_vector\",  # 필드 타입을 밀집 벡터로 설정\n",
    "            \"dims\": 4096,  # 벡터 차원 수 (예: BERT와 같은 모델에서 생성된 임베딩 차원)\n",
    "            \"index\": True,  # 색인 가능 여부 설정 (True로 설정하면 검색이 가능)\n",
    "              # 유사도 측정 방법 설정 (L2 정규화를 사용하여 유사도 계산)\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 index 생성\n",
    "def create_es_index(index, settings, mappings):\n",
    "    # 인덱스가 이미 존재하는지 확인\n",
    "    if es.indices.exists(index=index):\n",
    "        # 인덱스가 이미 존재하면 설정을 새로운 것으로 갱신하기 위해 삭제\n",
    "        es.indices.delete(index=index)\n",
    "    # 지정된 설정으로 새로운 인덱스 생성\n",
    "    es.indices.create(index=index, settings=settings, mappings=mappings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 지정된 인덱스 삭제\n",
    "def delete_es_index(index):\n",
    "    es.indices.delete(index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elasticsearch 헬퍼 함수를 사용하여 대량 인덱싱 수행\n",
    "def bulk_add(index, docs):\n",
    "    # 대량 인덱싱 작업을 준비\n",
    "    actions = [\n",
    "        {\n",
    "            '_index': index,\n",
    "            '_source': doc\n",
    "        }\n",
    "        for doc in docs\n",
    "    ]\n",
    "    return helpers.bulk(es, actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지정된 인덱스 삭제\n",
    "def delete_es_index(index):\n",
    "    es.indices.delete(index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 역색인을 이용한 검색\n",
    "def sparse_retrieve(query_str, size):\n",
    "    query = {\n",
    "        \"match\": {\n",
    "            \"content\": {\n",
    "                \"query\": query_str\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return es.search(index=\"test\", query=query, size=size, sort=\"_score\")\n",
    "\n",
    "\n",
    "def dense_retrieve(query_str, size):\n",
    "    # 벡터 유사도 검색에 사용할 쿼리 임베딩 가져오기\n",
    "    query_embedding = get_embedding([query_str])[0]\n",
    "\n",
    "    # KNN을 사용한 벡터 유사성 검색을 위한 매개변수 설정\n",
    "    knn = {\n",
    "        \"field\": \"embeddings\",\n",
    "        \"query_vector\": query_embedding.tolist(),\n",
    "        \"k\": size,\n",
    "        \"num_candidates\": 100\n",
    "    }\n",
    "    body={\n",
    "        \"size\" : size,\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\n",
    "                    \"match_all\": {}  # 모든 문서를 대상으로 검색\n",
    "                },\n",
    "                \"script\": {\n",
    "                    \"source\": \"cosineSimilarity(params.query_vector, 'embeddings') + 1.0\",\n",
    "                    \"params\": {\n",
    "                        \"k\" : size,\n",
    "                        \"query_vector\": query_embedding.tolist()\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # 지정된 인덱스에서 벡터 유사도 검색 수행\n",
    "    return es.search(index=\"test\", body=body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_es_index(\"test\", settings, mappings)\n",
    "\n",
    "# 문서의 content 필드에 대한 임베딩 생성\n",
    "index_docs = []\n",
    "with open(\"/home/data/documents.jsonl\") as f:\n",
    "    docs = [json.loads(line) for line in f]\n",
    "embeddings = get_embeddings_in_batches(docs)\n",
    "                \n",
    "# 생성한 임베딩을 색인할 필드로 추가\n",
    "for doc, embedding in zip(docs, embeddings):\n",
    "    doc[\"embeddings\"] = embedding.tolist()\n",
    "    index_docs.append(doc)\n",
    "\n",
    "# 'test' 인덱스에 대량 문서 추가\n",
    "ret = bulk_add(\"test\", index_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG 구현에 필요한 질의 분석 및 검색 이외의 일반 질의 대응을 위한 LLM 프롬프트\n",
    "persona_function_calling = \"\"\"\n",
    "## Role: 검색을 사용할 것인지, 대화를 생성할 것인 판단하는 역할 \n",
    "\n",
    "## Instruction\n",
    "* **주요 기능:** 사용자의 질문을 분석하여 관련된 정보를 검색하고 지식에 관련된 질문은 searchapi를 사용하여 검색을 하고, 일상 질문은 바로 생성합니다.\n",
    "* **검색 범위:** 인문학, 사회과학, 자연과학, 공학 등 모든 분야를 포괄합니다.\n",
    "* **검색 기준:**\n",
    "    * **관련성:** 사용자의 질문과 가장 관련성이 높은 정보를 우선적으로 제공합니다.\n",
    "    * **판단기준** 성적인 암시인 경우 같은 부적절한 것 같은 경우도 스스로 판단하지 말고, 검색이 가능하면 검색을 한다. \"우울하다\" 같이 감정을 나눈다면 \n",
    "    적절한 대답을 한국말로 생성한다.\n",
    "* **응답 형식:**\n",
    "    * **요약:** 검색 결과를 요약하여 간결하게 제공합니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "persona_qa = \"\"\"\n",
    "## Role: function_calling으로 생성된 답변을 최종적으로 판단하여 답을 한국말로 생성한다.\n",
    "\n",
    "## Instruction\n",
    "* **주요 기능:** 사용자와 자연스러운 대화를 이어가고, 다양한 주제에 대한 질문에 답변합니다.\n",
    "* **지식 기반:** `function_calling` 모듈을 통해 얻은 정보를 활용하여 정확한 답변을 제공합니다.\n",
    "* *감정 이해:** 사용자의 감정을 이해하고, 적절한 공감과 위로를 표현합니다.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = {\n",
    "  \"name\": \"search\",\n",
    "  \"description\": \"관련 문서를 검색합니다. 캐주얼한 대화를 제외한 모든 질문이나 요청 시 이 함수를 호출하세요. 예: '지구 자전의 원인은?', '세종대왕에 대해 알려줘.'\",\n",
    "  \"parameters\": {\n",
    "    \"type_\": \"OBJECT\",\n",
    "    \"properties\": {\n",
    "      \"standalone_query\": {\n",
    "        \"type_\": \"STRING\",\n",
    "        \"description\": \"사용자 메시지 기록을 기반으로 문서 검색에 적합한 최종 쿼리. 항상 한국어로 작성하세요.\"\n",
    "      }\n",
    "    },\n",
    "    \"required\": [\"standalone_query\"]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import traceback\n",
    "\n",
    "llm_model = genai.GenerativeModel('gemini-1.5-pro', tools=tools)\n",
    "genai.configure(api_key=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def answer_question(eval_filename, output_filename):\n",
    "    count = 0  \n",
    "    with open(eval_filename) as f, open(output_filename, \"w\") as of:\n",
    "        for line in f:\n",
    "            response = {\"standalone_query\": \"\", \"topk\": [], \"references\": [], \"answer\": \"\"}\n",
    "            j = json.loads(line)\n",
    "            last_j= j['msg'].pop()['parts']\n",
    "            chat = llm_model.start_chat(\n",
    "                history=j['msg']\n",
    "            )\n",
    "            try:\n",
    "                result = chat.send_message(last_j+persona_function_calling)\n",
    "            except Exception as e:\n",
    "                traceback.print_exc()\n",
    "                return response\n",
    "\n",
    "            if result.candidates[0].content.parts[0].function_call:\n",
    "                function_call = result.candidates[0].content.parts[0].function_call\n",
    "                standalone_query = function_call.args[\"standalone_query\"]\n",
    "                search_result_retrieve = dense_retrieve(standalone_query+\"?\", 3)\n",
    "                response[\"standalone_query\"] = standalone_query\n",
    "                retrieve_context = []\n",
    "                for i, rst in enumerate(search_result_retrieve['hits']['hits']):\n",
    "                    retrieve_context.append(rst[\"_source\"][\"content\"])\n",
    "                    response[\"topk\"].append(rst[\"_source\"][\"docid\"])\n",
    "                    response[\"references\"].append({\"score\": rst[\"_score\"], \"content\": rst[\"_source\"][\"content\"]})\n",
    "                    content = json.dumps(retrieve_context)\n",
    "                    j['msg'].append({\"role\": \"model\", \"parts\": content})\n",
    "                    last_j= j['msg'].pop()['parts']\n",
    "                try:\n",
    "                    qresult = chat.send_message(last_j+persona_qa)\n",
    "                except Exception as e:\n",
    "                    traceback.print_exc()\n",
    "                    response[\"answer\"] = qresult.candidates[0].content.parts[0].text\n",
    "                # print(response)\n",
    "        \n",
    "            else:\n",
    "                response[\"answer\"] = result.candidates[0].content.parts[0].text\n",
    "            count += 1  # 카운터 증가\n",
    "            output = {\"eval_id\": j[\"eval_id\"], \"standalone_query\": response[\"standalone_query\"], \"topk\": response[\"topk\"], \"answer\": response[\"answer\"], \"references\": response[\"references\"]}\n",
    "            of.write(f'{json.dumps(output, ensure_ascii=False)}\\n')\n",
    "            print(f\"Output {count}: {response}\")  # 출력 번호와 함께 출력\n",
    "    print(f\"Total number of outputs: {count}\")  # 총 출력 횟수 출력\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
